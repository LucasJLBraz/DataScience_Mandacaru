{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3ff366c3b75e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:26.175508323Z",
     "start_time": "2023-11-28T21:52:26.128196703Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29415ef175208a13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386994a697682f1e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Lendo o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4a5536ce387ee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:27.152024221Z",
     "start_time": "2023-11-28T21:52:27.070717091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "df = pd.read_csv(\"../Lucas Braz/data.csv\")\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(df.Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9caa56dd18a9bb7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Removendo linhas com valores nulos ou vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8297a3fcbb7fc56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:27.405941969Z",
     "start_time": "2023-11-28T21:52:27.388487195Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Remover linhas com valores nulos ou vazios\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56388b1eb36992",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Tonekenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd7f5d51ccc7460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:28.553516286Z",
     "start_time": "2023-11-28T21:52:27.674669753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the data into training, validation, and test sets\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(df\u001b[38;5;241m.\u001b[39mSentence\u001b[38;5;241m.\u001b[39mvalues, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mSentiment\u001b[38;5;241m.\u001b[39mvalues, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mX_temp, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2638\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2634\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2636\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2638\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2641\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2643\u001b[0m     )\n\u001b[1;32m   2644\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:1726\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1726\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2115\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2113\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2120\u001b[0m     )\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2126\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Init tokenizer\n",
    "# Bert has its own tokenizer which also include some special tokens\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Spliting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.Sentence.values, labels, test_size=.2, stratify=df.Sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7e7d6d06b2bc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89076da5b7f70be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:28.555730548Z",
     "start_time": "2023-11-28T21:52:28.555025753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data generator\n",
    "# Inherit Dataset from torch which need to implement __len__ and __getitem__ function\n",
    "\n",
    "class Datagen(Dataset):\n",
    "    def __init__(self, sentence, target, tokenizer, max_len):\n",
    "        self.sentence = sentence\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        bert_encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length = self.max_len,\n",
    "            add_special_tokens = True, # include special tokens\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True, # return attention mask which is required during training \n",
    "            return_token_type_ids = False,\n",
    "            return_tensors = 'pt' # pt is pytorch format tensor\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": torch.squeeze(bert_encoding[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.squeeze(bert_encoding[\"attention_mask\"]),\n",
    "            \"target\": torch.tensor(self.target[idx], dtype=torch.long)\n",
    "        }\n",
    "train_data = Datagen(X_train, y_train, tokenizer, 50)\n",
    "test_data = Datagen(X_test, y_test, tokenizer, 50)\n",
    "# DataLoader created batch generator and have prefatch ability\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=15, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a829df6b250bac3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Definindo o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bce38c022dd69d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:34.576533118Z",
     "start_time": "2023-11-28T21:52:31.431290084Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        # Here we are initializing some nn.Module attributes and functions\n",
    "        super(SentimentModel, self).__init__()\n",
    "        # Initializing bert model\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # bert model gives two outputs, sequenced output and pooled output\n",
    "        # Sequence output is last layer output for each token in sentence usualy used for similarity task\n",
    "        # Pooled output is cls token(starting token for each sentence) output from model used for classification task\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask, return_dict=False)\n",
    "        output = self.drop(pooled_output)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "model = SentimentModel(n_class=3)\n",
    "# Moving model to GPU\n",
    "model = model.to(torch.device('cuda'))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c078c65cb7381223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:34.577577597Z",
     "start_time": "2023-11-28T21:52:34.574514381Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_acc(preds, labels):\n",
    "    correct = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1716a3f2ff469f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:34.635447181Z",
     "start_time": "2023-11-28T21:52:34.579643743Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(dataloader, model):\n",
    "    model.eval()\n",
    "    batch_acc = 0\n",
    "    for inputs in dataloader:\n",
    "        to_cuda = lambda x: x.to(torch.device('cuda'), non_blocking=True).long()\n",
    "        input_ids, attention_mask, target = list(map(to_cuda, inputs.values()))\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        prediction = outputs.argmax(dim=1)\n",
    "        acc = check_acc(prediction, target)\n",
    "        batch_acc += acc\n",
    "    return batch_acc/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebd1bcd1b4d92f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d21b9eedcbfd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:52:21.086501546Z",
     "start_time": "2023-11-28T21:52:18.436223038Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Telling model to start training mode for dropout and BN\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(5):\n",
    "    # Telling model to start training mode for dropout and BN\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for steps, inputs in enumerate(train_dataloader):\n",
    "        to_cuda = lambda x: x.to(torch.device('cuda'), non_blocking=True).long()\n",
    "        # Moving variables to GPU\n",
    "        input_ids, attention_mask, target = list(map(to_cuda, inputs.values()))\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Initializing gradient to 0 so that it don't add up previous gradient        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the training statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted_labels == target).sum().item()\n",
    "        total_samples += target.size(0)\n",
    "\n",
    "        if steps % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Epoch: {epoch}, Batch: {steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and print average training loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / (steps + 1)\n",
    "    accuracy_epoch = correct_predictions / total_samples\n",
    "    print(f\"Epoch: {epoch}, Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy_epoch:.4f}\")\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_accuracy = accuracy(test_dataloader, model)\n",
    "    print(f\"Epoch: {epoch}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on validation accuracy\n",
    "    scheduler.step(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58156dd691acdfef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:35:26.221216799Z",
     "start_time": "2023-11-28T20:35:25.003263918Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "torch.save(model.state_dict(), \"bert_v1_semOtim.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa10b71bb0435a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# BERT com otimização e CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235bc892db44630",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 1: Imports e Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fb9064b0b50581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:41:01.190840302Z",
     "start_time": "2023-11-28T20:41:01.141493636Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Leitura dos dados\n",
    "df = pd.read_csv(\"../Lucas Braz/data.csv\")\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(df.Sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba660926b0f3b583",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 2: Pré-processamento e Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e1532c1672fa7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:41:02.620067675Z",
     "start_time": "2023-11-28T20:41:02.153882552Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Remover linhas com valores nulos ou vazios\n",
    "df = df.dropna()\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.Sentence.values, labels, test_size=.2, stratify=df.Sentiment.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8cd5cfbcada00",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 3: Definição da Classe de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5014d440f5ffa23e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:41:03.372320372Z",
     "start_time": "2023-11-28T20:41:03.349341097Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Datagen(Dataset):\n",
    "    def __init__(self, sentence, target, tokenizer, max_len):\n",
    "        self.sentence = sentence\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        bert_encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": torch.squeeze(bert_encoding[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.squeeze(bert_encoding[\"attention_mask\"]),\n",
    "            \"target\": torch.tensor(self.target[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b7e46233d9ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 4: Configuração do TensorBoard e Parâmetros do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7ea53a9f69ebc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:41:06.998340629Z",
     "start_time": "2023-11-28T20:41:04.760723748Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Configuração do TensorBoard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Modelo\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_class)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask, return_dict=False)\n",
    "        output = self.drop(pooled_output)\n",
    "        output = self.out(output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "model = SentimentModel(n_class=3)\n",
    "model = model.to(torch.device('cuda'))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58eee4f15a1d055",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 5: Funções Auxiliares e Treinamento com Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d23600f89f8656",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def check_acc(preds, labels):\n",
    "    correct = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct/len(preds)\n",
    "\n",
    "def accuracy(dataloader, model):\n",
    "    model.eval()\n",
    "    batch_acc = 0\n",
    "    for inputs in dataloader:\n",
    "        to_cuda = lambda x: x.to(torch.device('cuda'), non_blocking=True).long()\n",
    "        input_ids, attention_mask, target = list(map(to_cuda, inputs.values()))\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        prediction = outputs.argmax(dim=1)\n",
    "        acc = check_acc(prediction, target)\n",
    "        batch_acc += acc\n",
    "    return batch_acc/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bebe98fdb228cdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T20:58:11.441998563Z",
     "start_time": "2023-11-28T20:41:38.919942791Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1/5\t Epoch: 0\t Train accuracy: 0.8180460750853242\t Test accuracy: 0.756349206349206\n",
      "Fold 1/5\t Epoch: 1\t Train accuracy: 0.8191126279863481\t Test accuracy: 0.7486568986568983\n",
      "Fold 1/5\t Epoch: 2\t Train accuracy: 0.863481228668942\t Test accuracy: 0.7434676434676433\n",
      "Fold 1/5\t Epoch: 3\t Train accuracy: 0.8607081911262798\t Test accuracy: 0.7383394383394383\n",
      "Fold 1/5\t Epoch: 4\t Train accuracy: 0.875\t Test accuracy: 0.7357753357753355\n",
      "Fold 2/5\n",
      "Fold 2/5\t Epoch: 0\t Train accuracy: 0.8538822525597269\t Test accuracy: 0.8631257631257628\n",
      "Fold 2/5\t Epoch: 1\t Train accuracy: 0.8598549488054608\t Test accuracy: 0.8562881562881558\n",
      "Fold 2/5\t Epoch: 2\t Train accuracy: 0.8705204778156996\t Test accuracy: 0.873443223443223\n",
      "Fold 2/5\t Epoch: 3\t Train accuracy: 0.871160409556314\t Test accuracy: 0.8708791208791206\n",
      "Fold 2/5\t Epoch: 4\t Train accuracy: 0.8071672354948806\t Test accuracy: 0.815201465201465\n",
      "Fold 3/5\n",
      "Fold 3/5\t Epoch: 0\t Train accuracy: 0.8617747440273038\t Test accuracy: 0.8723865877712024\n",
      "Fold 3/5\t Epoch: 1\t Train accuracy: 0.8698805460750854\t Test accuracy: 0.8775147928994073\n",
      "Fold 3/5\t Epoch: 2\t Train accuracy: 0.8722269624573379\t Test accuracy: 0.873241288625903\n",
      "Fold 3/5\t Epoch: 3\t Train accuracy: 0.8741467576791809\t Test accuracy: 0.8775147928994074\n",
      "Fold 3/5\t Epoch: 4\t Train accuracy: 0.8686006825938567\t Test accuracy: 0.8732412886259028\n",
      "Fold 4/5\n",
      "Fold 4/5\t Epoch: 0\t Train accuracy: 0.8718003412969283\t Test accuracy: 0.892899408284023\n",
      "Fold 4/5\t Epoch: 1\t Train accuracy: 0.8760665529010239\t Test accuracy: 0.887771203155818\n",
      "Fold 4/5\t Epoch: 2\t Train accuracy: 0.8666808873720137\t Test accuracy: 0.8739644970414195\n",
      "Fold 4/5\t Epoch: 3\t Train accuracy: 0.8799061433447098\t Test accuracy: 0.8911900065746212\n",
      "Fold 4/5\t Epoch: 4\t Train accuracy: 0.8809726962457338\t Test accuracy: 0.8766600920447067\n",
      "Fold 5/5\n",
      "Fold 5/5\t Epoch: 0\t Train accuracy: 0.8848122866894198\t Test accuracy: 0.8800788954635104\n",
      "Fold 5/5\t Epoch: 1\t Train accuracy: 0.8822525597269625\t Test accuracy: 0.8673898750821822\n",
      "Fold 5/5\t Epoch: 2\t Train accuracy: 0.8856655290102389\t Test accuracy: 0.8767915844838916\n",
      "Fold 5/5\t Epoch: 3\t Train accuracy: 0.8877986348122867\t Test accuracy: 0.8705456936226161\n",
      "Fold 5/5\t Epoch: 4\t Train accuracy: 0.8451365187713311\t Test accuracy: 0.817817225509533\n"
     ]
    }
   ],
   "source": [
    "# Treinamento com validação cruzada\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "num_epochs = 5  # Substitua este valor pelo número desejado de épocas\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(df.Sentence.values, labels)):\n",
    "    print(f\"Fold {fold + 1}/{skf.get_n_splits()}\")\n",
    "\n",
    "    X_train, X_test = df.Sentence.values[train_index], df.Sentence.values[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    train_data = Datagen(X_train, y_train, tokenizer, 50)\n",
    "    test_data = Datagen(X_test, y_test, tokenizer, 50)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=16, num_workers=2)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=15, num_workers=2)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for steps, inputs in enumerate(train_dataloader):\n",
    "            to_cuda = lambda x: x.to(torch.device('cuda'), non_blocking=True).long()\n",
    "            input_ids, attention_mask, target = list(map(to_cuda, inputs.values()))\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        writer.add_scalar(f\"Train/Average_Loss_Fold{fold}\", average_loss, epoch)\n",
    "\n",
    "        train_acc = accuracy(train_dataloader, model)\n",
    "        test_acc = accuracy(test_dataloader, model)\n",
    "\n",
    "        print(f\"Fold {fold + 1}/{skf.get_n_splits()}\\t Epoch: {epoch}\\t Train accuracy: {train_acc}\\t Test accuracy: {test_acc}\")\n",
    "\n",
    "        writer.add_scalar(f\"Train/Accuracy_Fold{fold}\", train_acc, epoch)\n",
    "        writer.add_scalar(f\"Test/Accuracy_Fold{fold}\", test_acc, epoch)\n",
    "\n",
    "        scheduler.step(average_loss)\n",
    "\n",
    "# Salvando o modelo treinado\n",
    "torch.save(model.state_dict(), \"sentiment_model.pth\")\n",
    "\n",
    "# Fechando o TensorBoard\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870a22315507fc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853d6996b9dbbfa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
