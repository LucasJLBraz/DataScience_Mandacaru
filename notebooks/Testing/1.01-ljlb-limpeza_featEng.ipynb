{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a001cd5b05d142",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9abb3d87d35db",
   "metadata": {},
   "source": [
    "Segundo a importância das features, grande parte das features construídas apresentam importância significativa, logo, serão mantidas. Além disso, faremos o pré-processamento do texto, removendo stopwords, convertendo as palavras para seu lema, convertendo para minúsculas, a fim de simplificar o modelo, contendo apenas informação de quantas palavras em caixa alta estavam presente.\n",
    "\n",
    "Dentre os caracteres especiais, iremos remover todos, exceto interrogações e exclamações, pois apesar de não serem estritamente relevantes a maioria dos textos que tem sentimento positivo ou negativo, possuem uma maior quantidade desses caracteres. Assim, podemos considerar que eles podem carregar alguma informação relevante.\n",
    "\n",
    "Dessa forma, precisaremos extrair informações da sentença de forma mais granular, ou seja, palavra por palavra. Para isso, utilizaremos o TF-IDF (Term Frequency-Inverse Document Frequency), que é uma técnica de transformação de texto em vetores numéricos, onde cada palavra é representada por um número que indica sua importância no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db09179d54988a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T22:50:12.058151Z",
     "start_time": "2025-05-29T22:50:11.860367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário LM carregado: ../data/Loughran-McDonald_MasterDictionary_1993-2024.csv\n",
      "  Positive: 347 palavras\n",
      "  Negative: 2345 palavras\n",
      "  Uncertainty: 297 palavras\n",
      "  Litigious: 903 palavras\n",
      "  Strong_Modal: 19 palavras\n",
      "  Weak_Modal: 27 palavras\n",
      "  Constraining: 184 palavras\n",
      "\n",
      "Criando features...\n",
      "\n",
      "Usando dataset original: 5842 amostras\n",
      "\n",
      "Distribuição das classes:\n",
      "Sentiment\n",
      "neutral     3130\n",
      "positive    1852\n",
      "negative     860\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testando significância estatística...\n",
      "Testando 12 features (alpha=0.05)\n",
      "==================================================\n",
      "\n",
      "Feature: text_length\n",
      "  Kruskal-Wallis: H=98.386, p=0.0000 - Significativo\n",
      "\n",
      "Feature: word_count\n",
      "  Kruskal-Wallis: H=38.185, p=0.0000 - Significativo\n",
      "\n",
      "Feature: avg_word_length\n",
      "  Kruskal-Wallis: H=38.859, p=0.0000 - Significativo\n",
      "\n",
      "Feature: exclamation_count\n",
      "  Kruskal-Wallis: H=44.736, p=0.0000 - Significativo\n",
      "\n",
      "Feature: question_count\n",
      "  Kruskal-Wallis: H=25.719, p=0.0000 - Significativo\n",
      "\n",
      "Feature: uppercase_ratio\n",
      "  Kruskal-Wallis: H=348.114, p=0.0000 - Significativo\n",
      "\n",
      "Feature: lm_positive\n",
      "  Kruskal-Wallis: H=405.565, p=0.0000 - Significativo\n",
      "\n",
      "Feature: lm_negative\n",
      "  Kruskal-Wallis: H=538.983, p=0.0000 - Significativo\n",
      "\n",
      "Feature: lm_uncertainty\n",
      "  Kruskal-Wallis: H=1.910, p=0.3849 - Não significativo\n",
      "\n",
      "Feature: lm_litigious\n",
      "  Kruskal-Wallis: H=0.736, p=0.6921 - Não significativo\n",
      "\n",
      "Feature: lm_constraining\n",
      "  Kruskal-Wallis: H=13.237, p=0.0013 - Significativo\n",
      "\n",
      "Feature: lm_sentiment_score\n",
      "  Kruskal-Wallis: H=650.828, p=0.0000 - Significativo\n"
     ]
    }
   ],
   "source": [
    "df_processed, test_results = run_sentiment_analysis(\n",
    "    df=dataset_df,  # Seu DataFrame original\n",
    "    lm_file_path=LM_FILE_PATH,\n",
    "    use_balanced=False,  # Mude para False se não quiser balancear\n",
    "    print_plot=False  # Desativar plotagens para pré-processamento\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df26af4d0c1c72dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T22:50:12.611814Z",
     "start_time": "2025-05-29T22:50:12.086239Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remover alfanuméricos\n",
    "dataset_df['Sentence'] = dataset_df['Sentence'].str.replace(r'\\d+', '')\n",
    "\n",
    "# Deixar tudo em minúsculo\n",
    "dataset_df['Sentence'] = dataset_df['Sentence'].str.lower()\n",
    "\n",
    "# Remover pontuação, exceto exclamações e interrogações.\n",
    "dataset_df['Sentence'] = dataset_df['Sentence'].str.replace(r'[^\\w\\s!?]', '', regex=True)\n",
    "\n",
    "\n",
    "# Remover stopwords (Ex: \"the\", \"is\", \"in\", \"and\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "dataset_df['Sentence'] = dataset_df['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
    "\n",
    "# Leamatização (reduzir palavras à sua raiz)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dataset_df['Sentence'] = dataset_df['Sentence'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb9b4f9658f7652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T22:50:12.794322Z",
     "start_time": "2025-05-29T22:50:12.616776Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Primeiro fazer o train_test_split nos índices\n",
    "train_idx, test_idx = train_test_split(\n",
    "    dataset_df.index,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Separar os dados de treino e teste usando os índices\n",
    "X_train_text = dataset_df.loc[train_idx, 'Sentence']\n",
    "X_test_text = dataset_df.loc[test_idx, 'Sentence']\n",
    "\n",
    "\n",
    "X_train_num = dataset_df.loc[train_idx, numerical_features_names]\n",
    "X_test_num = dataset_df.loc[test_idx, numerical_features_names]\n",
    "\n",
    "# 3. Aplicar TF-IDF apenas no texto\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "# 4. Combinar TF-IDF com features numéricas\n",
    "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_num))\n",
    "\n",
    "# 4.1 Normalizar as features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_combined = scaler.fit_transform(X_train_combined)\n",
    "\n",
    "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_num))\n",
    "X_test_combined = scaler.transform(X_test_combined)\n",
    "\n",
    "y_train = dataset_df['Sentiment'].loc[train_idx]\n",
    "y_test = dataset_df['Sentiment'].loc[test_idx]\n",
    "\n",
    "\n",
    "# 6. Aplicar Label Encoder nos targets\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
